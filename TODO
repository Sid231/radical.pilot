
  + fix invokation frequency of idle callbacks
  + clean pilot cancelation
  + client agent startup
  + exec worker/watcher concurrency
  + lrms/scheduler dependency
  + profile names
  + log file name uniquenenss
  + state history (update ordering)
    + multiple updater instances will compete for state updates and push them
      out of order, thus screwing with the CU states which end up in the DB.\
    + limit to one update instance
  + pilot updates via updater (different pubsub channel)
  + make sure finalize is called on terminate (via atexit?)
  + make sure all profile entries exist, merge from module profiling branch
  + merge from devel
  + merge from osgng
  + forward tunnel endpoint from bootstrap_1 to agent
  + make sure opaque_slot is a dict
  + make sure that the lrms_config_hook is called only once
    (this is implicit now, as we only have one scheduler instance, and thus
    only one (set of) LRMS instances)
  - At this point it might be simpler to have one LRMS instance which handles
    MPI and non-MPI?  Not sure about the diversity of combinations which need
    supporting...
  = component failures need to propagate to the agent to trigger shutdown
  = torus scheduler is broken in the split arch
  - advance() implicitly publishes unit state, but the publisher needs to be
    declared explicitly.
  + rename 'worker' to 'sub-agent' to avoid confusion?
  + component: collapse declare_input and declare_worker
  + cu['_id'] -> cu['uid'] etc

  + state pubsub now has [cmd, ttype, thing] tuples fed from advance.
  + self.advance(cu, FAILED, ...) will not find a suitable output channel.
    Handle gracefully!  
  + Always push final things fully to DB!
  - we mostly don't need create() methods -- in many cases we can just use
    constructors.
  + default output staging component: __init__ calls Component __init__, but
    should call base __init__, which then calls Component __init__
    Same holds for other component implementations.
  - component's base.create calls should pick up all component implementations
    automatically, instead of having a static type map
  + Bridges should be created by session, both on client and on agent side.
  + on the component, add 'declare_thread' for actions which are needed more
    frequent than idle callbacks, and also need some guarantees on the
    frequency.  Prominent example: execution watcher threads.  Handle the
    'work()' loop as just one of those threads (the default one).
  - register_idle_callback -> register_timed_callback
  + component watcher needs to clean out component threads even for failed
    components
  + shutdown command propagation is broken

  - unit cancelation is best done on component level
    - subscribe to control to get cancel requests
    - for each cancel request
      - add to self._cancel_requested
    - whenver we see an advance or work request:
      - if uid in self._cancel_requested: advance(CANCEL) + drop   
    - we can still additionally add cancel actions during staging and
      execution...

  - use heartbeat mechanism also for bridges, sub_agents
  + define _cfg['heart'] and use that for heartbeat src instead of owner,
    limiting traffic and noise
  - Component.stop() should *first* unregister component_checker_cb 
    (and probably all other callbacks)
  - pilot default error cb calls sys.exit -> no shutdown.

  - fix all schedulers
  - state notifications should be obtained via a tailed cursor
  - update worker should check if a unit has multiple state updates pending. If
    so, only send the last one.  The client side cann still play callbacks for
    all intermediate states
  - update worker should not push state_host on state updates, only on full
    udpdates.
  - pilot failing during wait_units results in timeout.

  - the update worker should have a list of keys which it needs to push to
    MongoDB, and it should ignore other dict entries.  Alternatively, components
    should use underscore-prefixed keys for things which don't need to be stored
    in MongoDB.

  - separate single-process components from multi-processed, in terms of
    initialization
  - make sure we don't carry locks across fork calls.  If that turns out to be
    difficult (hi zmq), we have to execve after fork and start afresh.
  - idle_cb naming...
  - kill pilots on finalize_child
  + saga_pid in pmgr.launcher control_cb
  + heartbeat interval assertion
  - dying agent bridge causes agent shutdown to hang
  - lrms -> rm
  + dead in pilot watcher cb remains undetected
  - separation of controller.[uid,heart,owner] is unclean


Expected Termination Behavior:

  - session.close() at any point in time
    - in main thread:
      - close all umgrs
        - terminate all threads
        - cancel any non-final units and wait for them
          - advance them to CANCELED
          - issue 'cancel' commands (no reschedule)
          ? do we want notifications for the unit cancel?
          ? do we rely on UpdateWorker to register/store CANCELED advance?
        - stop all components (delegated to session controller)
        - stop all bridges    (delegated to session controller)
      - close all pmgrs
        - cancel any non-final pilots and wait for them
        ? do we want notifications for the pilot cancel?
        - terminate all threads
        - stop all components (delegated to session controller)
        - stop all bridges    (delegated to session controller)
      - terminate all threads
      - close session controller
        - terminate all threads
        - stop all bridges
        - stop all components
    - in other threads
      - as in main thread
      - thread should then exit (default), call sys.exit, or raise.
    - in other processes / components
      - stop the component
      - no message is sent, we expect the pmgr, umgr or session watcher to
        detect the closed component (if needed), and to start teardown 
        -> watcher calling session.close + raise
  - <Ctrl-C>:
    - application must except, call session.close()
  - pilot.exit_on_error
    session.close + raise

